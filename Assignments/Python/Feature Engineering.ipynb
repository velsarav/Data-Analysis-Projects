{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to learn how feature engineering improves model performance. \n",
    "\n",
    "Stpes:  \n",
    "1) Apply Discrete Fourier Transformation on the accelerometer sensor time series.    \n",
    "2) Transform the dataset from the time to the frequency domain.  \n",
    "3) Use a classification algorithm to create a model and submit the new predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your cloudant credentials go here\n",
    "# @hidden_cell\n",
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'password':\"\"\"\"\"\",\n",
    "  'custom_url':'',\n",
    "  'username':'',\n",
    "  'url':'https://undefined'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a SparkSession object and put the Cloudant credentials into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n",
    "    .config(\"cloudant.host\",credentials_1['custom_url'].split('@')[1])\\\n",
    "    .config(\"cloudant.username\", credentials_1['username'])\\\n",
    "    .config(\"cloudant.password\",credentials_1['password'])\\\n",
    "    .config(\"jsonstore.rdd.partitions\", 1)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to read the sensor data and create a temporary query table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.load('shake_classification', \"org.apache.bahir.cloudant\")\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure SystemML is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting systemml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/94/62104cb8c526b462cd501c7319926fb81ac9a5668574a0b3407658a506ab/systemml-1.2.0.tar.gz (9.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.7MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (1.13.3)\n",
      "Requirement already satisfied: scipy>=0.15.1 in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (1.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (0.19.1)\n",
      "Requirement already satisfied: Pillow>=2.0.0 in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (4.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->systemml) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->systemml) (2018.4)\n",
      "Requirement already satisfied: olefile in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from Pillow>=2.0.0->systemml) (0.44)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/src/conda3_runtime.v46/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from python-dateutil>=2->pandas->systemml) (1.11.0)\n",
      "Building wheels for collected packages: systemml\n",
      "  Running setup.py bdist_wheel for systemml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /gpfs/fs01/user/s2f3-1c0783c216973a-3e2ac3e59e61/.cache/pip/wheels/cf/07/79/b3ed6f12afe06b2ab55d60dcfd62e66240f5d8c6088a518177\n",
      "Successfully built systemml\n",
      "\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n",
      "\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "Installing collected packages: systemml\n",
      "Successfully installed systemml-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install systemml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemml import MLContext, dml\n",
    "ml = MLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you’ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn’t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R\n",
    "):\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1af0a78dc50bbf118ab6bd4c4dcc3c4ff8502223\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_script = '''\n",
    "PI = 3.141592654\n",
    "N = nrow(signal)\n",
    "\n",
    "n = seq(0, N-1, 1)\n",
    "k = seq(0, N-1, 1)\n",
    "\n",
    "M = (n %*% t(k))*(2*PI/N)\n",
    "\n",
    "Xa = cos(M) %*% signal\n",
    "Xb = sin(M) %*% signal\n",
    "\n",
    "DFT = cbind(Xa, Xb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def dft_systemml(signal,name):\n",
    "    prog = dml(dml_script).input('signal', signal).output('DFT')\n",
    "    \n",
    "    return (\n",
    "\n",
    "    #execute the script inside the SystemML engine running on top of Apache Spark\n",
    "    ml.execute(prog) \n",
    "     \n",
    "         #read result from SystemML execution back as SystemML Matrix\n",
    "        .get('DFT') \n",
    "     \n",
    "         #convert SystemML Matrix to ApacheSpark DataFrame \n",
    "        .toDF() \n",
    "     \n",
    "         #rename default column names\n",
    "        .selectExpr('C1 as %sa' % (name), 'C2 as %sb' % (name)) \n",
    "     \n",
    "         #add unique ID per row for later joining\n",
    "        .withColumn(\"id\", monotonically_increasing_id())\n",
    "    )\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+-----+-----+--------------------+--------------------+\n",
      "|CLASS|SENSORID|    X|    Y|    Z|                 _id|                _rev|\n",
      "+-----+--------+-----+-----+-----+--------------------+--------------------+\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert|-0.01|-0.01|-0.01|020533cb000ab3de5...|1-df37baeaa4c3926...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert| 0.01| 0.01| 0.01|020533cb000ab3de5...|1-3e653d871d6c029...|\n",
      "|    0|velqwert|-0.02|-0.02|-0.02|020533cb000ab3de5...|1-f3902329a0f1765...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert| 0.01| 0.01| 0.01|020533cb000ab3de5...|1-3e653d871d6c029...|\n",
      "|    0|velqwert|-0.01|-0.01|-0.01|020533cb000ab3de5...|1-df37baeaa4c3926...|\n",
      "|    0|velqwert| 0.02| 0.02| 0.02|020533cb000ab3de5...|1-ca1c2f3e827f972...|\n",
      "|    0|velqwert| 0.02| 0.02| 0.02|020533cb000ab3de5...|1-ca1c2f3e827f972...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert| 0.01| 0.01| 0.01|020533cb000ab3de5...|1-3e653d871d6c029...|\n",
      "|    0|velqwert|-0.01|-0.01|-0.01|020533cb000ab3de5...|1-df37baeaa4c3926...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert| 0.07| 0.07| 0.07|020533cb000ab3de5...|1-9763ce9b44d16e5...|\n",
      "|    0|velqwert|  0.0|  0.0|  0.0|020533cb000ab3de5...|1-f5e2391b0458648...|\n",
      "|    0|velqwert|-0.01|-0.01|-0.01|020533cb000ab3de5...|1-df37baeaa4c3926...|\n",
      "+-----+--------+-----+-----+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create DataFrames containing for each accelerometer sensor axis and one for each class. This means you’ll get 6 DataFrames. Please implement this using the relational API of DataFrames or SparkSQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT class,X from df where class = 1\").show()\n",
    "x0 = spark.sql(\"SELECT class,X from df where class = 0\")\n",
    "y0 = spark.sql(\"SELECT class,Y from df where class = 0\")\n",
    "z0 = spark.sql(\"SELECT class,Z from df where class = 0\")\n",
    "x1 = spark.sql(\"SELECT class,X from df where class = 1\")\n",
    "y1 = spark.sql(\"SELECT class,Y from df where class = 1\")\n",
    "z1 = spark.sql(\"SELECT class,Z from df where class = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n",
    "\n",
    "- Calling DFT for each class and accelerometer sensor axis.\n",
    "- Joining them together on the ID column. \n",
    "- Re-adding a column containing the class index.\n",
    "- Stacking both Dataframes for each classes together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemML Statistics:\n",
      "Total execution time:\t\t1.873 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t1.062 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.976 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.980 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t1.044 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t1.018 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+\n",
      "|        id| xa|                  xb| ya|                  yb| za|                  zb|class|\n",
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+\n",
      "|        26|0.0|  3.4659961767723746|0.0|  3.4659961767723746|0.0|  3.4659961767723746|    0|\n",
      "|        29|0.0|  -6.067001873042779|0.0|  -6.067001873042779|0.0|  -6.067001873042779|    0|\n",
      "|       474|0.0|  -4.773707646336872|0.0|  -4.773707646336872|0.0|  -4.773707646336872|    0|\n",
      "|       964|0.0|  -1.668821675169658|0.0|  -1.668821675169658|0.0|  -1.668821675169658|    0|\n",
      "|8589934658|0.0| 0.12590268819396355|0.0| 0.12590268819396355|0.0| 0.12590268819396355|    0|\n",
      "|8589934965|0.0|   1.054783961509861|0.0|   1.054783961509861|0.0|   1.054783961509861|    0|\n",
      "|8589935171|0.0|-0.41915698669272866|0.0|-0.41915698669272866|0.0|-0.41915698669272866|    0|\n",
      "|8589935183|0.0|  0.9408678028287316|0.0|  0.9408678028287316|0.0|  0.9408678028287316|    0|\n",
      "|8589935298|0.0| -1.5473892494685135|0.0| -1.5473892494685135|0.0| -1.5473892494685135|    0|\n",
      "|8589935317|0.0|  0.9396234074864053|0.0|  0.9396234074864053|0.0|  0.9396234074864053|    0|\n",
      "|        65|0.0|   3.843941767058578|0.0|   3.843941767058578|0.0|   3.843941767058578|    0|\n",
      "|       191|0.0|  1.2718896135316318|0.0|  1.2718896135316318|0.0|  1.2718896135316318|    0|\n",
      "|       418|0.0|  1.4979572574025877|0.0|  1.4979572574025877|0.0|  1.4979572574025877|    0|\n",
      "|       541|0.0| -0.3813328049917911|0.0| -0.3813328049917911|0.0| -0.3813328049917911|    0|\n",
      "|       558|0.0|  1.0375179839601858|0.0|  1.0375179839601858|0.0|  1.0375179839601858|    0|\n",
      "|8589934785|0.0|  0.5567884145156277|0.0|  0.5567884145156277|0.0|  0.5567884145156277|    0|\n",
      "|8589934903|0.0|  0.8654091841624266|0.0|  0.8654091841624266|0.0|  0.8654091841624266|    0|\n",
      "|8589935056|0.0| -1.8887874790531856|0.0| -1.8887874790531856|0.0| -1.8887874790531856|    0|\n",
      "|8589935196|0.0|  1.1481717543682337|0.0|  1.1481717543682337|0.0|  1.1481717543682337|    0|\n",
      "|8589935211|0.0|   -4.65296507361385|0.0|   -4.65296507361385|0.0|   -4.65296507361385|    0|\n",
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_class_0 = dft_systemml(x0,'x') \\\n",
    "    .join(dft_systemml(y0,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z0,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(0))\n",
    "    \n",
    "df_class_1 = dft_systemml(x1,'x') \\\n",
    "    .join(dft_systemml(y1,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z1,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(1))\n",
    "\n",
    "df_dft = df_class_0.union(df_class_1)\n",
    "\n",
    "df_dft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a VectorAssembler which consumes the newly created DFT columns and produces a column “features”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"xa\",\"xb\",\"ya\",\"yb\",\"za\",\"zb\"],\n",
    "                                  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insatiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to set the “class” column as target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "classifier = GBTClassifier(labelCol=\"class\", featuresCol=\"features\",maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train and evaluate…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+--------------------+----------+\n",
      "|        id| xa|                  xb| ya|                  yb| za|                  zb|class|            features|prediction|\n",
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+--------------------+----------+\n",
      "|        26|0.0|  3.4659961767723746|0.0|  3.4659961767723746|0.0|  3.4659961767723746|    0|[0.0,3.4659961767...|       0.0|\n",
      "|        29|0.0|  -6.067001873042779|0.0|  -6.067001873042779|0.0|  -6.067001873042779|    0|[0.0,-6.067001873...|       0.0|\n",
      "|       474|0.0|  -4.773707646336872|0.0|  -4.773707646336872|0.0|  -4.773707646336872|    0|[0.0,-4.773707646...|       0.0|\n",
      "|       964|0.0|  -1.668821675169658|0.0|  -1.668821675169658|0.0|  -1.668821675169658|    0|[0.0,-1.668821675...|       0.0|\n",
      "|8589934658|0.0| 0.12590268819396355|0.0| 0.12590268819396355|0.0| 0.12590268819396355|    0|[0.0,0.1259026881...|       0.0|\n",
      "|8589934965|0.0|   1.054783961509861|0.0|   1.054783961509861|0.0|   1.054783961509861|    0|[0.0,1.0547839615...|       0.0|\n",
      "|8589935171|0.0|-0.41915698669272866|0.0|-0.41915698669272866|0.0|-0.41915698669272866|    0|[0.0,-0.419156986...|       0.0|\n",
      "|8589935183|0.0|  0.9408678028287316|0.0|  0.9408678028287316|0.0|  0.9408678028287316|    0|[0.0,0.9408678028...|       0.0|\n",
      "|8589935298|0.0| -1.5473892494685135|0.0| -1.5473892494685135|0.0| -1.5473892494685135|    0|[0.0,-1.547389249...|       0.0|\n",
      "|8589935317|0.0|  0.9396234074864053|0.0|  0.9396234074864053|0.0|  0.9396234074864053|    0|[0.0,0.9396234074...|       0.0|\n",
      "|        65|0.0|   3.843941767058578|0.0|   3.843941767058578|0.0|   3.843941767058578|    0|[0.0,3.8439417670...|       0.0|\n",
      "|       191|0.0|  1.2718896135316318|0.0|  1.2718896135316318|0.0|  1.2718896135316318|    0|[0.0,1.2718896135...|       0.0|\n",
      "|       418|0.0|  1.4979572574025877|0.0|  1.4979572574025877|0.0|  1.4979572574025877|    0|[0.0,1.4979572574...|       0.0|\n",
      "|       541|0.0| -0.3813328049917911|0.0| -0.3813328049917911|0.0| -0.3813328049917911|    0|[0.0,-0.381332804...|       0.0|\n",
      "|       558|0.0|  1.0375179839601858|0.0|  1.0375179839601858|0.0|  1.0375179839601858|    0|[0.0,1.0375179839...|       0.0|\n",
      "|8589934785|0.0|  0.5567884145156277|0.0|  0.5567884145156277|0.0|  0.5567884145156277|    0|[0.0,0.5567884145...|       0.0|\n",
      "|8589934903|0.0|  0.8654091841624266|0.0|  0.8654091841624266|0.0|  0.8654091841624266|    0|[0.0,0.8654091841...|       0.0|\n",
      "|8589935056|0.0| -1.8887874790531856|0.0| -1.8887874790531856|0.0| -1.8887874790531856|    0|[0.0,-1.888787479...|       0.0|\n",
      "|8589935196|0.0|  1.1481717543682337|0.0|  1.1481717543682337|0.0|  1.1481717543682337|    0|[0.0,1.1481717543...|       0.0|\n",
      "|8589935211|0.0|   -4.65296507361385|0.0|   -4.65296507361385|0.0|   -4.65296507361385|    0|[0.0,-4.652965073...|       0.0|\n",
      "+----------+---+--------------------+---+--------------------+---+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"class\")\n",
    "    \n",
    "binEval.evaluate(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction value is 1.0 which is better than the last two assignment predictions, feature engineering improves the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment submission  \n",
    "If the value is greater than 0.8, please submit your solution to the grader by executing the following cells, please don’t forget to obtain an assignment submission token (secret) from the Courera’s graders web page and paste it to the “secret” variable below, including your email address you’ve used for Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf a2_m4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.repartition(1)\n",
    "prediction.write.json('a2_m4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-18 16:58:32--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2029 (2.0K) [text/plain]\n",
      "Saving to: ‘rklib.py’\n",
      "\n",
      "100%[======================================>] 2,029       --.-K/s   in 0s      \n",
      "\n",
      "2018-10-18 16:58:32 (16.0 MB/s) - ‘rklib.py’ saved [2029/2029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: a2_m4.json/ (stored 0%)\n",
      "  adding: a2_m4.json/_SUCCESS (stored 0%)\n",
      "  adding: a2_m4.json/._SUCCESS.crc (stored 0%)\n",
      "  adding: a2_m4.json/part-00000-edc82fd7-d3af-40af-9901-0b86cc89303c.json (deflated 87%)\n",
      "  adding: a2_m4.json/.part-00000-edc82fd7-d3af-40af-9901-0b86cc89303c.json.crc (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r a2_m4.json.zip a2_m4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!base64 a2_m4.json.zip > a2_m4.json.zip.base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful, please check on the coursera grader page for the status\n"
     ]
    }
   ],
   "source": [
    "from rklib import submit\n",
    "key = \"-fBiYHYDEeiR4QqiFhAvkA\"\n",
    "part = \"IjtJk\"\n",
    "email = \"\"\n",
    "secret = \"jFvyHgc6EQLgn8wC\"\n",
    "\n",
    "with open('a2_m4.json.zip.base64', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "submit(email, secret, key, part, [part], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
